{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ADB Phase 2 Project Evaluation Notebook\n"
      ],
      "metadata": {
        "id": "Yj9-3U--Krvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: This notebook evaluates the performance of a semantic search project by analyzing databases of various sizes.\n",
        "\n",
        "### Evaluation Focus:\n",
        "- **Database Sizes**:\n",
        "  - 1 Million Records\n",
        "  - 10 Million Records\n",
        "  - 20 Million Records\n",
        "\n",
        "For each database size, this notebook will:\n",
        "- Download the database\n",
        "- Use the `VecDB` class (implemented by students) to retrieve queries\n",
        "- Evaluate and report retrieval time, accuracy, and RAM usage.\n",
        "\n",
        "### Project Constraints:\n",
        "Refer to the project document for details on RAM, Disk, Time, and Score constraints.\n",
        "\n",
        "### Notebook Structure:\n",
        "1. **Part 1 - Modifiable Cells**:\n",
        "   - Includes cells that teams are allowed to modify, specifically for these variables only:\n",
        "     - GitHub repository link (including PAT token).\n",
        "     - Google Drive IDs for indexes files.\n",
        "     - Paths for loading existing indexes.\n",
        "\n",
        "2. **Part 2 - Non-Modifiable Cells**:\n",
        "   - Contains essential setup and evaluation code that must not be modified.\n",
        "   - Students should only modify inputs in Part 1 to ensure smooth execution of the notebook."
      ],
      "metadata": {
        "id": "hV2Nc_f8Mbqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Modifiable Cells"
      ],
      "metadata": {
        "id": "C4EV_xB6Kw17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each team must provide a unique GitHub repository link that includes a PAT token. This link will allow the notebook to download the necessary code for evaluation."
      ],
      "metadata": {
        "id": "AODP-iztLtBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCR6Z8ABxE3w",
        "outputId": "49a12348-5282-409b-a907-146293767ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vec_db'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 139 (delta 24), reused 25 (delta 11), pack-reused 97 (from 1)\u001b[K\n",
            "Receiving objects: 100% (139/139), 57.08 KiB | 885.00 KiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/amrmgdy02/vec_db.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database Path Instructions\n"
      ],
      "metadata": {
        "id": "O7QSIX510KMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teams need to specify paths for each database (1M, 10M, 20M records) as follows:\n",
        "\n",
        "1. Zip each database directory/file after generation.\n",
        "2. Upload the zip file to Google Drive.\n",
        "3. Share the file with \"Anyone with the link.\"\n",
        "4. Extract the file ID from the link (e.g., for `https://drive.google.com/file/d/1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah/view`, the ID is `1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah`).\n",
        "5. Assign each ID to the appropriate variable in Part 1.\n",
        "6. Provide the local PATH for each database to be passed to the initializer for automatic loading of the database and index (to be submitted during the project final phase). (This path could be folder name or whatever string you need).\n",
        "\n",
        "**Note**: The code will download and unzip these files automatically. Once extracted, the local path for each database should be specified to enable the notebook to load databases and indexes."
      ],
      "metadata": {
        "id": "UsUXWYom6xRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEAM_NUMBER = 9\n",
        "GDRIVE_ID_DB_1M = \"1dU73VNBVJRK384z2AKXSTSpGM3qc04VE\"\n",
        "GDRIVE_ID_DB_10M = \"15ya5-QQ9deUE3j-C3XiSuHvRzzW7G1PE\"\n",
        "GDRIVE_ID_DB_20M = \"1lU4mLxhdR9cnfmmODpHtI-SYQxdPEVpD\"\n",
        "PATH_DB_1M = \"OpenSubtitles_en_1M_index_64\"\n",
        "PATH_DB_10M = \"OpenSubtitles_en_10M_index_64\"\n",
        "PATH_DB_20M = \"OpenSubtitles_en_20M_index_64\""
      ],
      "metadata": {
        "id": "kK46_ZVe5L3u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seed Number**:\n",
        "This number will be changed during discussions by the instructor.\n"
      ],
      "metadata": {
        "id": "0LGLg01fsujm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED_NUMBER = 10\n",
        "import random\n",
        "random.seed(SEED_NUMBER)"
      ],
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Submission Checklist**:\n",
        "Ensure the following items are included in your final submission:\n",
        "- `TEAM_NUMBER`\n",
        "- GitHub clone link (with PAT token)\n",
        "- Google Drive IDs for each database:\n",
        "  - `GDRIVE_ID_DB_1M`, `GDRIVE_ID_DB_10M`, `GDRIVE_ID_DB_20M`\n",
        "- Paths for each database:\n",
        "  - `PATH_DB_1M`, `PATH_DB_10M`, `PATH_DB_20M`\n",
        "- Project document detailing the work and findings."
      ],
      "metadata": {
        "id": "kWaZ-ByWOIcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Do Not Modify Beyond This Point\n",
        "### Note:\n",
        "This section contains setup and evaluation code that should not be edited by students. Only the instructor may modify this section in case of a major bug.\n"
      ],
      "metadata": {
        "id": "hzFTOecwu8wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is not working now for some reason on Colab\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "67NUn3KWXA6u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd vec_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqujj7tYTA1l",
        "outputId": "a45dd24f-21d7-448d-dfc8-463686e16f1a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vec_db/vec_db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ],
      "metadata": {
        "id": "yJmXzFdisD7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler >> log.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "3ff10179-3ff4-4ba4-c7a0-98172a0397ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.16.3)\n",
            "Requirement already satisfied: memory-profiler in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.61.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory-profiler->-r requirements.txt (line 4)) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell to download the zip files and unzip them here."
      ],
      "metadata": {
        "id": "lG0DALR498__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown $GDRIVE_ID_DB_1M -O saved_db_1m.zip\n",
        "!gdown $GDRIVE_ID_DB_10M -O saved_db_10m.zip\n",
        "!gdown $GDRIVE_ID_DB_20M -O saved_db_20m.zip\n",
        "!unzip saved_db_1m.zip\n",
        "!unzip saved_db_10m.zip\n",
        "!unzip saved_db_20m.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSv2z0PVp6HA",
        "outputId": "b0e25853-ffaa-4aa3-c874-9abe7d414f21"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dU73VNBVJRK384z2AKXSTSpGM3qc04VE\n",
            "To: /content/vec_db/vec_db/saved_db_1m.zip\n",
            "100% 17.0M/17.0M [00:00<00:00, 32.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15ya5-QQ9deUE3j-C3XiSuHvRzzW7G1PE\n",
            "From (redirected): https://drive.google.com/uc?id=15ya5-QQ9deUE3j-C3XiSuHvRzzW7G1PE&confirm=t&uuid=cdb2bbfa-06d3-4c80-9b68-2673f4271fa7\n",
            "To: /content/vec_db/vec_db/saved_db_10m.zip\n",
            "100% 59.6M/59.6M [00:00<00:00, 64.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1lU4mLxhdR9cnfmmODpHtI-SYQxdPEVpD\n",
            "From (redirected): https://drive.google.com/uc?id=1lU4mLxhdR9cnfmmODpHtI-SYQxdPEVpD&confirm=t&uuid=060b7362-de6d-4222-8520-e0025852de22\n",
            "To: /content/vec_db/vec_db/saved_db_20m.zip\n",
            "100% 125M/125M [00:00<00:00, 126MB/s]\n",
            "Archive:  saved_db_1m.zip\n",
            "   creating: OpenSubtitles_en_1M_index_64/\n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_inverted_ids.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_inverted_offsets.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_ivf_centroids.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_metadata.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_opq_rotation.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_pq_codebooks.npy  \n",
            "  inflating: OpenSubtitles_en_1M_index_64/OpenSubtitles_en_1M_pq_codes.dat  \n",
            "Archive:  saved_db_10m.zip\n",
            "   creating: OpenSubtitles_en_10M_index_64/\n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_inverted_ids.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_inverted_offsets.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_ivf_centroids.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_metadata.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_opq_rotation.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_pq_codebooks.npy  \n",
            "  inflating: OpenSubtitles_en_10M_index_64/OpenSubtitles_en_10M_pq_codes.dat  \n",
            "Archive:  saved_db_20m.zip\n",
            "   creating: OpenSubtitles_en_20M_index_64/\n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_inverted_ids.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_inverted_offsets.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_ivf_centroids.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_metadata.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_opq_rotation.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_pq_codebooks.npy  \n",
            "  inflating: OpenSubtitles_en_20M_index_64/OpenSubtitles_en_20M_pq_codes.dat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and Generate The DBs"
      ],
      "metadata": {
        "id": "wp19a7NgJtmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "usqLi0C8K1h6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_DB_VECTORS_20M = \"OpenSubtitles_en_20M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_10M = \"OpenSubtitles_en_10M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_1M = \"OpenSubtitles_en_1M_emb_64.dat\"\n",
        "if not os.path.exists(PATH_DB_VECTORS_20M):\n",
        "    !gdown \"1gy2KRFCY3M4uToK-hnf6Vv5VFsOgqfgb\" -O \"OpenSubtitles_en_20M_emb_64.dat\""
      ],
      "metadata": {
        "id": "zfAitTOIJtQD",
        "outputId": "f5403a07-3a95-4125-c454-3f717aeed3f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gy2KRFCY3M4uToK-hnf6Vv5VFsOgqfgb\n",
            "From (redirected): https://drive.google.com/uc?id=1gy2KRFCY3M4uToK-hnf6Vv5VFsOgqfgb&confirm=t&uuid=bf213cfc-8581-43ae-8556-dd5a9238db02\n",
            "To: /content/vec_db/vec_db/OpenSubtitles_en_20M_emb_64.dat\n",
            "100% 5.12G/5.12G [01:42<00:00, 49.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DIMENSION = 64\n",
        "def create_other_DB_size(input_file, output_file, target_rows, embedding_dim = DIMENSION):\n",
        "    # Configuration\n",
        "    dtype = 'float32'\n",
        "\n",
        "    # 1. Determine the shape of the source file\n",
        "    # We calculate rows based on file size to be safe, or you can hardcode 20_000_000\n",
        "    file_size_bytes = os.path.getsize(input_file)\n",
        "    itemsize = np.dtype(dtype).itemsize\n",
        "    total_rows = file_size_bytes // (embedding_dim * itemsize)\n",
        "\n",
        "    print(f\"Source detected: {total_rows} rows.\")\n",
        "\n",
        "    # 2. Open source in read mode ('r')\n",
        "    # This uses almost 0 RAM, it just points to the file on disk\n",
        "    source_memmap = np.memmap(\n",
        "        input_file,\n",
        "        dtype=dtype,\n",
        "        mode='r',\n",
        "        shape=(total_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 3. Create the new file in write mode ('w+')\n",
        "    # We define the shape as the target size (1M, 64)\n",
        "    dest_memmap = np.memmap(\n",
        "        output_file,\n",
        "        dtype=dtype,\n",
        "        mode='w+',\n",
        "        shape=(target_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 4. Copy the data\n",
        "    # This transfers the binary blocks directly\n",
        "    print(\"Copying data...\")\n",
        "    dest_memmap[:] = source_memmap[:target_rows]\n",
        "\n",
        "    # 5. Flush to save changes to disk\n",
        "    dest_memmap.flush()\n",
        "\n",
        "    print(f\"Success! Saved first {target_rows} rows to {output_file}\")"
      ],
      "metadata": {
        "id": "2SPhELotKXaO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(PATH_DB_VECTORS_1M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_1M, 1_000_000)\n",
        "if not os.path.exists(PATH_DB_VECTORS_10M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_10M, 10_000_000)"
      ],
      "metadata": {
        "id": "_1uvNlMELYk0",
        "outputId": "f45af7bc-0d2a-401d-ea9b-3b24d1928ac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source detected: 20000000 rows.\n",
            "Copying data...\n",
            "Success! Saved first 1000000 rows to OpenSubtitles_en_1M_emb_64.dat\n",
            "Source detected: 20000000 rows.\n",
            "Copying data...\n",
            "Success! Saved first 10000000 rows to OpenSubtitles_en_10M_emb_64.dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to generate the queries that will be used to evaluate the questions.\n",
        "\n",
        "Note: English sentences will be changed at submission day\n",
        "\n",
        "The first sentence will be used just for warmup, then the others will be used for evaluation"
      ],
      "metadata": {
        "id": "Q4GDcog3dMZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries_embed_file = \"queries_emb_64.dat\"\n",
        "\n",
        "if not os.path.exists(queries_embed_file):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    batch_sentences = [\n",
        "        \"Hello World\",\n",
        "        \"We are Software Engineering Students\",\n",
        "        \"What's the best way to be a good human?\",\n",
        "        \"What a good day\"\n",
        "    ]\n",
        "    model = SentenceTransformer('minishlab/potion-base-2M')\n",
        "    queries_np = model.encode(batch_sentences, convert_to_numpy=True)\n",
        "    queries_np = queries_np.astype(np.float32)\n",
        "    queries_np.tofile(queries_embed_file)\n",
        "else:\n",
        "    queries_np = np.fromfile(queries_embed_file, dtype=np.float32).reshape(-1, DIMENSION)\n",
        "\n",
        "query_dummy = queries_np[0].reshape(1, DIMENSION)\n",
        "queries = [queries_np[1].reshape(1, DIMENSION), queries_np[2].reshape(1, DIMENSION), queries_np[3].reshape(1, DIMENSION)]\n",
        "queries_np = queries_np[1:,:]"
      ],
      "metadata": {
        "id": "xIPNJ3nWPErt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the sorted_ids for each DB"
      ],
      "metadata": {
        "id": "Gi9E370vaMGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_sorted_ids_file = \"actual_sorted_ids_20m.dat\"\n",
        "saved_top_k = 30_000\n",
        "needed_top_k = 10_000\n",
        "if not os.path.exists(actual_sorted_ids_file):\n",
        "    vectors = np.memmap(PATH_DB_VECTORS_20M, dtype='float32', mode='r', shape=(20_000_000, DIMENSION))\n",
        "    actual_sorted_ids_20m = np.argsort(np.dot(vectors, queries_np.T) / (1e-45 + np.linalg.norm(vectors, axis=1)[:, None] * np.linalg.norm(queries_np, axis=1)), axis=0)[-saved_top_k:][::-1].T\n",
        "    actual_sorted_ids_20m = actual_sorted_ids_20m.astype(np.int32)\n",
        "    actual_sorted_ids_20m.tofile(actual_sorted_ids_file)\n",
        "else:\n",
        "    actual_sorted_ids_20m = np.fromfile(actual_sorted_ids_file, dtype=np.int32).reshape(-1, saved_top_k)"
      ],
      "metadata": {
        "id": "ad_K9Zz9bZ8m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the functions for running and reporting"
      ],
      "metadata": {
        "id": "ShuPR-gGlX3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k, out_len = 10_000):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids][:out_len]"
      ],
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code to actually run the class you have been implemented. The `VecDB` class should take the database path, and index path that you provided.<br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB"
      ],
      "metadata": {
        "id": "rrOlipAOmy9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check memory usage for the import line independently\n",
        "import tracemalloc\n",
        "tracemalloc.start()\n",
        "start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "from vec_db import VecDB\n",
        "\n",
        "end_snapshot = tracemalloc.take_snapshot()\n",
        "stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "for stat in stats[:5]:  # show top differences\n",
        "    print(stat)\n",
        "\n",
        "tracemalloc.stop()"
      ],
      "metadata": {
        "id": "Yi34qMLHsqCn",
        "outputId": "3b0a3252-8a2a-4af1-b22b-c0cecb5912b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.12/tracemalloc.py:560: size=192 B (+192 B), count=2 (+2), average=96 B\n",
            "/usr/lib/python3.12/tracemalloc.py:423: size=192 B (+192 B), count=2 (+2), average=96 B\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3473: size=312 B (+0 B), count=1 (+0), average=312 B\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3358: size=287 B (+0 B), count=2 (+0), average=144 B\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3553: size=160 B (+0 B), count=1 (+0), average=160 B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ],
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "database_info = {\n",
        "    \"1M\": {\n",
        "        \"database_file_path\": PATH_DB_VECTORS_1M,\n",
        "        \"index_file_path\": PATH_DB_1M,\n",
        "        \"size\": 10**6\n",
        "    },\n",
        "    \"10M\": {\n",
        "        \"database_file_path\": PATH_DB_VECTORS_10M,\n",
        "        \"index_file_path\": PATH_DB_10M,\n",
        "        \"size\": 10 * 10**6\n",
        "    },\n",
        "    \"20M\": {\n",
        "        \"database_file_path\": PATH_DB_VECTORS_20M,\n",
        "        \"index_file_path\": PATH_DB_20M,\n",
        "        \"size\": 20 * 10**6\n",
        "    }\n",
        "}\n",
        "\n",
        "for db_name, info in database_info.items():\n",
        "    print(f\"*\"*40)\n",
        "    print(f\"Evaluating DB of size {db_name}\")\n",
        "\n",
        "    # This part added to check RAM usage for the class init function\n",
        "    tracemalloc.start()\n",
        "    start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "\n",
        "    end_snapshot = tracemalloc.take_snapshot()\n",
        "    stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "    for stat in stats[:5]:  # show top differences\n",
        "        print(stat)\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"], needed_top_k)\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    # CRITICAL DON'T CACHE ANYTHING IN THE QUERY FUNCTION\n",
        "\n",
        "    # This part added to check RAM usage for the run queries with another method\n",
        "    tracemalloc.start()\n",
        "    start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "\n",
        "    end_snapshot = tracemalloc.take_snapshot()\n",
        "    stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "    for stat in stats[:5]:  # show top differences\n",
        "        print(stat)\n",
        "    tracemalloc.stop()\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-UFbhBPlQtz",
        "outputId": "015d6433-6770-4690-f52d-f3291baef3aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team Number 9\n",
            "****************************************\n",
            "Evaluating DB of size 1M\n",
            "/tmp/ipython-input-361132729.py:28: size=288 B (+288 B), count=2 (+2), average=144 B\n",
            "/content/vec_db/vec_db.py:59: size=288 B (+288 B), count=2 (+2), average=144 B\n",
            "/content/vec_db/vec_db.py:58: size=288 B (+288 B), count=2 (+2), average=144 B\n",
            "/content/vec_db/vec_db.py:57: size=288 B (+288 B), count=2 (+2), average=144 B\n",
            "/usr/lib/python3.12/tracemalloc.py:560: size=168 B (+168 B), count=2 (+2), average=84 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:836: size=341 KiB (+341 KiB), count=20 (+20), average=17.0 KiB\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py:302: size=3008 B (+3008 B), count=47 (+47), average=64 B\n",
            "/usr/lib/python3.12/ast.py:74: size=896 B (+896 B), count=7 (+7), average=128 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:644: size=800 B (+800 B), count=20 (+20), average=40 B\n",
            "/usr/lib/python3.12/ast.py:86: size=800 B (+800 B), count=5 (+5), average=160 B\n",
            "1M\tscore\t0.0\ttime\t0.38\tRAM\t20.00 MB\n",
            "****************************************\n",
            "Evaluating DB of size 10M\n",
            "/content/vec_db/vec_db.py:59: size=400 B (+400 B), count=4 (+4), average=100 B\n",
            "/content/vec_db/vec_db.py:57: size=400 B (+400 B), count=3 (+3), average=133 B\n",
            "/content/vec_db/vec_db.py:58: size=344 B (+344 B), count=3 (+3), average=115 B\n",
            "/tmp/ipython-input-361132729.py:28: size=280 B (+280 B), count=2 (+2), average=140 B\n",
            "/usr/lib/python3.12/tracemalloc.py:423: size=184 B (+184 B), count=3 (+3), average=61 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:836: size=1121 KiB (+1121 KiB), count=23 (+23), average=48.7 KiB\n",
            "/content/vec_db/vec_db.py:170: size=24.9 KiB (+24.9 KiB), count=456 (+456), average=56 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py:302: size=3520 B (+3520 B), count=55 (+55), average=64 B\n",
            "/usr/lib/python3.12/ast.py:74: size=896 B (+896 B), count=7 (+7), average=128 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:644: size=800 B (+800 B), count=20 (+20), average=40 B\n",
            "10M\tscore\t-11.0\ttime\t0.99\tRAM\t23.40 MB\n",
            "****************************************\n",
            "Evaluating DB of size 20M\n",
            "/content/vec_db/vec_db.py:57: size=392 B (+392 B), count=3 (+3), average=131 B\n",
            "/content/vec_db/vec_db.py:59: size=336 B (+336 B), count=3 (+3), average=112 B\n",
            "/content/vec_db/vec_db.py:58: size=336 B (+336 B), count=3 (+3), average=112 B\n",
            "/tmp/ipython-input-361132729.py:28: size=272 B (+272 B), count=2 (+2), average=136 B\n",
            "/usr/lib/python3.12/tracemalloc.py:423: size=144 B (+144 B), count=3 (+3), average=48 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:836: size=4241 KiB (+4241 KiB), count=18 (+18), average=236 KiB\n",
            "/content/vec_db/vec_db.py:170: size=24.9 KiB (+24.9 KiB), count=455 (+455), average=56 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py:302: size=4416 B (+4416 B), count=69 (+69), average=64 B\n",
            "/usr/lib/python3.12/ast.py:74: size=896 B (+896 B), count=7 (+7), average=128 B\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py:644: size=800 B (+800 B), count=20 (+20), average=40 B\n",
            "20M\tscore\t-306.0\ttime\t0.39\tRAM\t22.60 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "print(\"\\n\".join(to_print_arr))"
      ],
      "metadata": {
        "id": "jt1_7ihfB37Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bc94a8-34ac-4293-fe5d-71425f1cbd94"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team Number 9\n",
            "1M\tscore\t0.0\ttime\t0.38\tRAM\t20.00 MB\n",
            "10M\tscore\t-11.0\ttime\t0.99\tRAM\t23.40 MB\n",
            "20M\tscore\t-306.0\ttime\t0.39\tRAM\t22.60 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git log"
      ],
      "metadata": {
        "id": "XXdMoPfVsUgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00c20d6-cf9b-4e49-8c1c-82185c84eb38"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mcommit ee48bd0f083048f391943b2b0b524431a2dfad6b\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Fri Dec 5 04:13:48 2025 +0200\n",
            "\n",
            "    static metadata for each db size\n",
            "\n",
            "\u001b[33mcommit b714b1540b1c561180264669987dd6577b65fa72\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Fri Dec 5 03:39:00 2025 +0200\n",
            "\n",
            "    dynamic M\n",
            "\n",
            "\u001b[33mcommit 49522042bd90df96b47932b07075f7a1d54a253f\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Fri Dec 5 01:10:57 2025 +0200\n",
            "\n",
            "    change M to match index file\n",
            "\n",
            "\u001b[33mcommit d320dd79ee9ed9283d4b9533e4690e94a2b2d08c\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Dec 4 21:26:10 2025 +0200\n",
            "\n",
            "    test path\n",
            "\n",
            "\u001b[33mcommit a91353f2ec44582f7f2be352e02694cd41d3873f\u001b[m\n",
            "Merge: 2dfae19 3de2c12\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Dec 4 21:16:31 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit 3de2c126e319b02c966dcc07f3932637ae717d97\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Thu Dec 4 20:46:33 2025 +0200\n",
            "\n",
            "    deleted cached\n",
            "\n",
            "\u001b[33mcommit 2dfae198e731f89aa3815115c53e57353d0728f7\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Dec 4 20:32:48 2025 +0200\n",
            "\n",
            "    ignore\n",
            "\n",
            "\u001b[33mcommit 5926f0b1c34b5f522e11d550ec5c19790d0ee0e1\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Dec 4 20:32:43 2025 +0200\n",
            "\n",
            "    signle index path\n",
            "\n",
            "\u001b[33mcommit 0668816a186ad90dcef6699859156651a3a9a99d\u001b[m\n",
            "Merge: 8dfd176 ef3812d\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Sat Nov 29 11:45:40 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit 8dfd1762357dd7af684a6a6f4bee8f293a3fc2db\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Sat Nov 29 11:45:27 2025 +0200\n",
            "\n",
            "    ignored index files\n",
            "\n",
            "\u001b[33mcommit ef3812da3165301bd76d83a4c0691ef4eb02aab2\u001b[m\n",
            "Author: MohamedMaher03 <m7mdrefaat550@gmail.com>\n",
            "Date:   Sat Nov 29 11:14:58 2025 +0200\n",
            "\n",
            "    update DB params\n",
            "\n",
            "\u001b[33mcommit 04f30fba9b9d3d2bc598e348e7a91f7c5ff17ac5\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Fri Nov 28 22:53:09 2025 +0200\n",
            "\n",
            "    Fixed a bug with updated index names and eval function\n",
            "\n",
            "\u001b[33mcommit 16484028eeb0c3f93fb6bb47795fed718ad258ca\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Fri Nov 28 20:40:46 2025 +0200\n",
            "\n",
            "    - Evaluation with RAM\n",
            "    - Custom IVF sample size and index files names\n",
            "\n",
            "\u001b[33mcommit 9599978c84e57073b95aad5b9762af5f5b2f5110\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Fri Nov 28 03:33:31 2025 +0200\n",
            "\n",
            "    20M run params\n",
            "\n",
            "\u001b[33mcommit 503d8771920e51139e4b9afd34280fc3423752a7\u001b[m\n",
            "Merge: 3a285f8 df34507\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Thu Nov 27 23:44:14 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit 3a285f87efccf9ced1971148c893da880b708e62\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Thu Nov 27 23:42:53 2025 +0200\n",
            "\n",
            "    Reranking added to the retrieval\n",
            "\n",
            "\u001b[33mcommit df34507d428f61abe68c321060c2da1fb7c4d7aa\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 22:39:56 2025 +0200\n",
            "\n",
            "    optimized assign\n",
            "\n",
            "\u001b[33mcommit 36370d548b25a95a36d4f7335821fe0bcfb545d6\u001b[m\n",
            "Merge: 3076b02 25c2a3e\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 20:56:57 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit 3076b0256efb9f05ec0ef2b177a838e8393e62e2\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 20:56:52 2025 +0200\n",
            "\n",
            "    added normalization\n",
            "\n",
            "\u001b[33mcommit 25c2a3ec20d234e803896c65ee118fd319efeca2\u001b[m\n",
            "Merge: 01741e1 99005a3\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Thu Nov 27 20:07:44 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit 01741e18aee1d80eb280334d3c7a88276e699d62\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Thu Nov 27 20:06:21 2025 +0200\n",
            "\n",
            "    ignore file\n",
            "\n",
            "\u001b[33mcommit 99005a323f79e4596755e766cd2297d52e2134d6\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 17:47:40 2025 +0200\n",
            "\n",
            "    gitignore\n",
            "\n",
            "\u001b[33mcommit eb8764cdb6cfcfe5bff36b440e765db9bfbe1618\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 17:47:33 2025 +0200\n",
            "\n",
            "    retrieve without pq\n",
            "\n",
            "\u001b[33mcommit 6f62038f7a0f9883b04cf34fdbd2f5ac51d9fa90\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Thu Nov 27 02:57:39 2025 +0200\n",
            "\n",
            "    change to .npy files to allow memmap\n",
            "    added load index function\n",
            "\n",
            "\u001b[33mcommit 47538b6db452c564a1e5a5969faab3f7561d6100\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Wed Nov 26 18:27:06 2025 +0200\n",
            "\n",
            "    save separate index files (may be combined later)\n",
            "\n",
            "\u001b[33mcommit a17b9b95729e562f81c2aa2ba5db0e12a7c2339c\u001b[m\n",
            "Merge: d828b1a c775b9b\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Wed Nov 26 18:07:05 2025 +0200\n",
            "\n",
            "    Merge branch 'main' of https://github.com/amrmgdy02/Semantic-Search-Engine\n",
            "\n",
            "\u001b[33mcommit c775b9ba34535366e1902b6d48ac23195839bcdb\u001b[m\n",
            "Author: MahmoudAly <mahmoudaly964@gmail.com>\n",
            "Date:   Wed Nov 26 17:09:06 2025 +0200\n",
            "\n",
            "    change build index to use trained data not rotated\n",
            "\n",
            "\u001b[33mcommit d828b1ada749cebfbd4c50d48a466401dc087fdc\u001b[m\n",
            "Author: Amr Magdy <129008521+amrmgdy02@users.noreply.github.com>\n",
            "Date:   Wed Nov 26 13:37:27 2025 +0200\n",
            "\n",
            "    fix in save & load\n",
            "\n",
            "\u001b[33mcommit 9d5bc7d6464e6a6086319838c59afbab37ad9ba8\u001b[m\n",
            "Author: MahmoudAly <mahmoudaly964@gmail.com>\n",
            "Date:   Tue Nov 25 16:02:31 2025 +0200\n",
            "\n",
            "    implement ivf and retrieve but still need potimization\n",
            "\n",
            "\u001b[33mcommit 2ca7d27a00d08d66505f288a311e01e2a9c556e5\u001b[m\n",
            "Author: Abdelrahman Ayman <abdelrahman.akefafy@gmail.com>\n",
            "Date:   Sat Nov 22 22:24:55 2025 +0200\n",
            "\n",
            "    Optimized Product Quantization Implemented\n",
            "\n",
            "\u001b[33mcommit 045748bcf0c99f95a52fd3d000c9ea455fbf6593\u001b[m\n",
            "Author: MohamedMaher03 <m7mdrefaat550@gmail.com>\n",
            "Date:   Thu Nov 20 22:59:29 2025 +0200\n",
            "\n",
            "    make Product Quantization class + implement adc_distance + implement temp  _build_index in vec_db\n",
            "\n",
            "\u001b[33mcommit e49ca10456f26673bd8110105c715a104bf490a6\u001b[m\n",
            "Author: abdokaseb <abdokaseb@gmail.com>\n",
            "Date:   Sun Nov 10 18:15:07 2024 +0200\n",
            "\n",
            "    Edited the eval file\n",
            "\n",
            "\u001b[33mcommit 34f274169391bc9fcab5b674f2ad7d663b33b220\u001b[m\n",
            "Author: abdokaseb <abdokaseb@gmail.com>\n",
            "Date:   Sat Nov 9 17:58:15 2024 +0200\n",
            "\n",
            "    Add vec_db file. Finalize VecDB methods signatures\n",
            "\n",
            "\u001b[33mcommit 66e43baea9cecf3c14ea4aab6457f1fb22d82c60\u001b[m\n",
            "Author: farah-moh <75073858+farah-moh@users.noreply.github.com>\n",
            "Date:   Wed Nov 6 23:59:37 2024 +0200\n",
            "\n",
            "    Update and rename README to README.md\n",
            "\n",
            "\u001b[33mcommit 19ea60bb8133ab58f21fc63821cfd077c6e1e2cb\u001b[m\n",
            "Author: farah-moh <75073858+farah-moh@users.noreply.github.com>\n",
            "Date:   Wed Nov 6 23:56:00 2024 +0200\n",
            "\n",
            "    Update README\n",
            "\n",
            "\u001b[33mcommit 08e8e0f24cbf3bbff281a19c5acfeec22ffaab14\u001b[m\n",
            "Author: farah-moh <farah.mohammed01@eng-st.cu.edu.eg>\n",
            "Date:   Wed Nov 6 23:55:21 2024 +0200\n",
            "\n",
            "    project new files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h --max-depth=2"
      ],
      "metadata": {
        "id": "naecE8DNBZLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b0ff51-dd36-434c-ee94-d94befe80990"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78M\t./OpenSubtitles_en_10M_index_64\n",
            "157M\t./OpenSubtitles_en_20M_index_64\n",
            "20M\t./OpenSubtitles_en_1M_index_64\n",
            "32K\t./.git/logs\n",
            "80K\t./.git/objects\n",
            "64K\t./.git/hooks\n",
            "28K\t./.git/refs\n",
            "4.0K\t./.git/branches\n",
            "8.0K\t./.git/info\n",
            "240K\t./.git\n",
            "7.9G\t.\n"
          ]
        }
      ]
    }
  ]
}